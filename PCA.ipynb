{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62d35a3c",
   "metadata": {},
   "source": [
    "# What is Dimensionality Reduction?\n",
    "In machine learning problems, there are often too many factors on the basis of which the final classification is done. These factors are basically variables called features. The higher the number of features, the harder it gets to visualize the training set and then work on it. The process of selecting a subset of features for use in model construction is called Dimensionality Reduction.\n",
    "\n",
    "Before Learning the techniques of Dimensionality Reduction, lets understand why it is important to do Dimensionality Reduction in our Dataset.\n",
    "\n",
    "Reasons :\n",
    "1) The abundance of redundant and irrelevant features\n",
    "\n",
    "2) With a fixed number of training samples, the predictive power reduces as the dimensionality increases. [Hughes phenomenon]\n",
    "\n",
    "3) Other things being equal, simpler explanations are generally better than complex ones.\n",
    "\n",
    "4) It improves the accuracy of a model if the right subset is chosen.\n",
    "\n",
    "5) Reduces the Overfitting.\n",
    "\n",
    "6) It reduces computation time.\n",
    "\n",
    "7) It helps in data compression, and hence reduced storage space.\n",
    "\n",
    "# Dimensionality Reduction Techniques\n",
    "\n",
    "1) Percent missing values\n",
    "\n",
    "2) Amount of variation\n",
    "\n",
    "3) Multicollinearity\n",
    "\n",
    "4) Principal Component Analysis (PCA)\n",
    "\n",
    "5) Correlation (with the target)\n",
    "\n",
    "6) Forward selection\n",
    "\n",
    "7) Backward elimination\n",
    "\n",
    "8) LASSO\n",
    "\n",
    "# Percent Missing values\n",
    "Drop variables/features that have a very high % of missing values.\n",
    "Review or visualize variables with high % of missing values\n",
    "\n",
    "# Amount of Variation\n",
    "Drop variables that have a very low variation.\n",
    "Either standardize all variables, or use standard deviation ùúé to account for variables with difference scales.\n",
    "Drop variables with zero variation.\n",
    "\n",
    "# Multicollinearity\n",
    "Many variables are often correlated with each other, and hence are redundant.\n",
    "If two or more variables are highly correlated, keeping only one will help reduce dimensionality without much loss of information.\n",
    "Which variable to keep? The one that has a higher correlation coefficient with the target.\n",
    "\n",
    "# Principal Component Analysis (PCA)\n",
    "PCA is defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.\n",
    "Dimensionality reduction technique which emphasizes variation.\n",
    "\n",
    "# When to use:\n",
    "Excessive multicollinearity\n",
    "Explanation of the predictors is not important.\n",
    "\n",
    "# Correlation (with the target)\n",
    "Drop variables that have a very low correlation with the target.\n",
    "If a variable has a very low correction with the target, it‚Äôs not going to useful for the model (prediction).\n",
    "\n",
    "# Forward selection\n",
    "Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model.\n",
    "Identify the best variable. (e.g., based on model accuracy)\n",
    "Add the next best variable into the model.\n",
    "And so on until some predefined criteria is satisfied.\n",
    "\n",
    "# Backward elimination\n",
    "In backward elimination, we start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features.\n",
    "Start with all variables included in the model.\n",
    "Drop the least useful variable (e.g., based on the smallest drop in model accuracy)\n",
    "And so on until some predefined criteria is satisfied.\n",
    "\n",
    "# LASSO\n",
    "Using Linear Regression with L1 regularization is called Lasso Regularization.\n",
    "The LASSO method puts a constraint on the sum of the absolute values of the model parameters, the sum has to be less than a fixed value (upper bound). In order to do so the method apply a shrinking (regularization) process where it penalizes the coefficients of the regression variables shrinking some of them to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f92217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e83689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59241cf4",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "Principal Component Analysis, or PCA for short, is a method for reducing the dimensionality of data.\n",
    "\n",
    "It can be thought of as a projection method where data with m-columns (features) is projected into a subspace with m or fewer columns, whilst retaining the essence of the original data.\n",
    "\n",
    "The PCA method can be described and implemented using the tools of linear algebra.\n",
    "\n",
    "PCA is an operation applied to a dataset, represented by an n x m matrix A that results in a projection of A which we will call B. Let‚Äôs walk through the steps of this operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06af8edf",
   "metadata": {},
   "source": [
    "\n",
    "     a11, a12\n",
    "A = (a21, a22)\n",
    "     a31, a32\n",
    " \n",
    "B = PCA(A)\n",
    "# The first step is to calculate the mean values of each column.\n",
    "\n",
    "M = mean(A)\n",
    "or\n",
    "\n",
    "              (a11 + a21 + a31) / 3\n",
    "M(m11, m12) = (a12 + a22 + a32) / 3\n",
    "\n",
    "# Next, we need to center the values in each column by subtracting the mean column value.\n",
    "\n",
    "C = A - M\n",
    "\n",
    "# The next step is to calculate the covariance matrix of the centered matrix C.\n",
    "\n",
    "Correlation is a normalized measure of the amount and direction (positive or negative) that two columns change together. Covariance is a generalized and unnormalized version of correlation across multiple columns. A covariance matrix is a calculation of covariance of a given matrix with covariance scores for every column with every other column, including itself.\n",
    "\n",
    "V = cov(C)\n",
    "\n",
    "# Finally, we calculate the eigendecomposition of the covariance matrix V. This results in a list of eigenvalues and a list of eigenvectors.\n",
    "\n",
    "values, vectors = eig(V)\n",
    "\n",
    "# The eigenvectors represent the directions or components for the reduced subspace of B, whereas the eigenvalues represent the magnitudes for the directions. For more on this topic, see the post:\n",
    "\n",
    "# Gentle Introduction to Eigendecomposition, Eigenvalues, and Eigenvectors for Machine Learning\n",
    "The eigenvectors can be sorted by the eigenvalues in descending order to provide a ranking of the components or axes of the new subspace for A.\n",
    "\n",
    "# If all eigenvalues have a similar value, then we know that the existing representation may already be reasonably compressed or dense and that the projection may offer little. If there are eigenvalues close to zero, they represent components or axes of B that may be discarded.\n",
    "\n",
    "A total of m or less components must be selected to comprise the chosen subspace. Ideally, we would select k eigenvectors, called principal components, that have the k largest eigenvalues.\n",
    "\n",
    "B = select(values, vectors)\n",
    "\n",
    "Other matrix decomposition methods can be used such as Singular-Value Decomposition, or SVD. As such, generally the values are referred to as singular values and the vectors of the subspace are referred to as principal components.\n",
    "\n",
    "Once chosen, data can be projected into the subspace via matrix multiplication.\n",
    "\n",
    "P = B^T . A\n",
    "\n",
    "Where A is the original data that we wish to project, B^T is the transpose of the chosen principal components and P is the projection of A.\n",
    "\n",
    "This is called the covariance method for calculating the PCA, although there are alternative ways to to calculate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c37fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[3. 4.]\n",
      "[[-2. -2.]\n",
      " [ 0.  0.]\n",
      " [ 2.  2.]]\n",
      "[[4. 4.]\n",
      " [4. 4.]]\n",
      "[[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n",
      "[8. 0.]\n",
      "[[-2.82842712  0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 2.82842712  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from numpy import array\n",
    "from numpy import mean\n",
    "from numpy import cov\n",
    "from numpy.linalg import eig\n",
    "# define a matrix\n",
    "A = array([[1, 2], [3, 4], [5, 6]])\n",
    "print(A)\n",
    "# calculate the mean of each column\n",
    "M = mean(A.T, axis=1)\n",
    "print(M)\n",
    "# center columns by subtracting column means\n",
    "C = A - M\n",
    "print(C)\n",
    "# calculate covariance matrix of centered matrix\n",
    "V = cov(C.T)\n",
    "print(V)\n",
    "# eigendecomposition of covariance matrix\n",
    "values, vectors = eig(V)\n",
    "print(vectors)\n",
    "print(values)\n",
    "# project data\n",
    "P = vectors.T.dot(C.T)\n",
    "print(P.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17af61c",
   "metadata": {},
   "source": [
    "# using SKlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59b6af43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[[ 0.70710678  0.70710678]\n",
      " [ 0.70710678 -0.70710678]]\n",
      "[8.00000000e+00 2.25080839e-33]\n",
      "[[-2.82842712e+00  2.22044605e-16]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [ 2.82842712e+00 -2.22044605e-16]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from sklearn.decomposition import PCA\n",
    "# define a matrix\n",
    "A = array([[1, 2], [3, 4], [5, 6]])\n",
    "print(A)\n",
    "# create the PCA instance\n",
    "pca = PCA(2)\n",
    "# fit on data\n",
    "pca.fit(A)\n",
    "# access values and vectors\n",
    "print(pca.components_)\n",
    "print(pca.explained_variance_)\n",
    "# transform data\n",
    "B = pca.transform(A)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d577db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a335354d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
